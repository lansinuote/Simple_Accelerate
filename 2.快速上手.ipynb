{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3865d825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 62 2.29443359375 9.990133642141359e-05 0.15625\n",
      "1 62 1.952362060546875 9.96057350657239e-05 0.34375\n",
      "2 62 1.7434158325195312 9.911436253643445e-05 0.4375\n",
      "3 62 1.5019912719726562 9.842915805643155e-05 0.46875\n",
      "4 62 1.1470603942871094 9.755282581475769e-05 0.5625\n",
      "5 62 0.8336296081542969 9.648882429441257e-05 0.78125\n",
      "6 62 0.4786376953125 9.524135262330098e-05 1.0\n",
      "7 62 0.32486724853515625 9.381533400219318e-05 1.0\n",
      "8 62 0.354766845703125 9.221639627510076e-05 0.96875\n",
      "9 62 0.19248199462890625 9.045084971874738e-05 1.0\n",
      "10 62 0.11542224884033203 8.852566213878947e-05 1.0\n",
      "11 62 0.08633613586425781 8.644843137107059e-05 1.0\n",
      "12 62 0.23552846908569336 8.422735529643444e-05 0.90625\n",
      "13 62 0.1086416244506836 8.18711994874345e-05 0.96875\n",
      "14 62 0.07680702209472656 7.938926261462366e-05 1.0\n",
      "15 62 0.061490535736083984 7.679133974894983e-05 1.0\n",
      "16 62 0.07937812805175781 7.408768370508576e-05 1.0\n",
      "17 62 0.11747360229492188 7.128896457825364e-05 1.0\n",
      "18 62 0.048834800720214844 6.840622763423391e-05 1.0\n",
      "19 62 0.031154632568359375 6.545084971874738e-05 1.0\n",
      "20 62 0.026040315628051758 6.243449435824276e-05 1.0\n",
      "21 62 0.023905277252197266 5.9369065729286245e-05 1.0\n",
      "22 62 0.021758556365966797 5.6266661678215216e-05 1.0\n",
      "23 62 0.017981529235839844 5.313952597646568e-05 1.0\n",
      "24 62 0.015864133834838867 5e-05 1.0\n",
      "25 62 0.01406717300415039 4.6860474023534335e-05 1.0\n",
      "26 62 0.013277292251586914 4.373333832178478e-05 1.0\n",
      "27 62 0.012686014175415039 4.063093427071376e-05 1.0\n",
      "28 62 0.010268568992614746 3.756550564175727e-05 1.0\n",
      "29 62 0.009793877601623535 3.4549150281252636e-05 1.0\n",
      "30 62 0.009238243103027344 3.1593772365766105e-05 1.0\n",
      "31 62 0.010110735893249512 2.8711035421746367e-05 1.0\n",
      "32 62 0.009002685546875 2.591231629491423e-05 1.0\n",
      "33 62 0.008426785469055176 2.3208660251050158e-05 1.0\n",
      "34 62 0.0075609683990478516 2.061073738537635e-05 1.0\n",
      "35 62 0.008436918258666992 1.8128800512565513e-05 1.0\n",
      "36 62 0.007865190505981445 1.5772644703565565e-05 1.0\n",
      "37 62 0.008193492889404297 1.3551568628929434e-05 1.0\n",
      "38 62 0.007358431816101074 1.1474337861210543e-05 1.0\n",
      "39 62 0.007588744163513184 9.549150281252633e-06 1.0\n",
      "40 62 0.006699562072753906 7.783603724899257e-06 1.0\n",
      "41 62 0.006855368614196777 6.184665997806832e-06 1.0\n",
      "42 62 0.00681757926940918 4.758647376699032e-06 1.0\n",
      "43 62 0.006823420524597168 3.511175705587433e-06 1.0\n",
      "44 62 0.006361246109008789 2.4471741852423237e-06 1.0\n",
      "45 62 0.006762266159057617 1.5708419435684462e-06 1.0\n",
      "46 62 0.006847977638244629 8.856374635655695e-07 1.0\n",
      "47 62 0.006472945213317871 3.9426493427611177e-07 1.0\n",
      "48 62 0.0063152313232421875 9.866357858642205e-08 1.0\n",
      "49 62 0.006753206253051758 0.0 1.0\n",
      "50 62 0.006597697734832764 9.866357858642205e-08 1.0\n",
      "51 62 0.0062427520751953125 3.942649342761062e-07 1.0\n",
      "52 62 0.006509065628051758 8.856374635655695e-07 1.0\n",
      "53 62 0.006317853927612305 1.5708419435684462e-06 1.0\n",
      "54 62 0.006567120552062988 2.4471741852423237e-06 1.0\n",
      "55 62 0.006627917289733887 3.5111757055874383e-06 1.0\n",
      "56 62 0.00643002986907959 4.758647376699016e-06 1.0\n",
      "57 62 0.006403326988220215 6.1846659978068155e-06 1.0\n",
      "58 62 0.006282925605773926 7.78360372489924e-06 1.0\n",
      "59 62 0.006214618682861328 9.549150281252623e-06 1.0\n",
      "60 62 0.006386280059814453 1.1474337861210538e-05 1.0\n",
      "61 62 0.006461620330810547 1.3551568628929422e-05 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=16, microseconds=699176)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from functions import get_loader, get_model\n",
    "from accelerate import Accelerator\n",
    "import datetime\n",
    "\n",
    "_, _, loader = get_loader()\n",
    "model, optimizer, scheduler = get_model()\n",
    "\n",
    "#使用4步梯度累积,梯度累计和mixed_precision不共存,不知道是否是一个bug\n",
    "# accelerator = Accelerator(gradient_accumulation_steps=4)\n",
    "\n",
    "#可以修改这里的mixed_precision,来查看不同精度的时间差,显存差\n",
    "#no,fp8,fp16,bf16\n",
    "accelerator = Accelerator(gradient_accumulation_steps=1,\n",
    "                          mixed_precision='fp16')\n",
    "\n",
    "loader, model, optimizer, scheduler = accelerator.prepare(\n",
    "    loader, model, optimizer, scheduler)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "for i, data in enumerate(loader):\n",
    "    #在这个范围内累积梯度\n",
    "    with accelerator.accumulate(model):\n",
    "        out = model(**data)\n",
    "        accelerator.backward(out.loss)\n",
    "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "        labels = data['labels']\n",
    "        logits = out['logits'].argmax(1)\n",
    "        acc = (labels == logits).sum().item() / len(labels)\n",
    "\n",
    "        print(i, len(loader), out.loss.item(), lr, acc)\n",
    "\n",
    "datetime.datetime.now() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f38d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "f runed\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "\n",
    "def f():\n",
    "    print('f runed')\n",
    "\n",
    "\n",
    "#在jupyter中也可以这样运行,主要就是可以增加一些参数\n",
    "notebook_launcher(f, num_processes=0, mixed_precision='fp16')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
